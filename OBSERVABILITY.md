# üìä Observability Stack - DX03

Stack completa de observabilidade para aplica√ß√£o DX03 rodando no GKE Autopilot.

## üéØ Overview

A stack de observabilidade do DX03 combina Prometheus, Grafana e Google Cloud Monitoring para fornecer visibilidade completa da aplica√ß√£o e infraestrutura.

### Componentes Implementados

- **Prometheus** - Coleta e armazenamento de m√©tricas
- **Grafana** - Visualiza√ß√£o e dashboards
- **Alertmanager** - Gerenciamento e notifica√ß√µes de alertas
- **Kube State Metrics** - M√©tricas de recursos do Kubernetes
- **Google Cloud Monitoring** - M√©tricas de nodes e infraestrutura GKE
- **prom-client** - Instrumenta√ß√£o do backend Node.js

## üöÄ Deploy

### Via GitHub Actions Workflow

```bash
# Install (primeira vez)
gh workflow run deploy-observability.yml --field action=install

# Upgrade (atualizar)
gh workflow run deploy-observability.yml --field action=upgrade

# Uninstall (remover)
gh workflow run deploy-observability.yml --field action=uninstall
```

### Secrets Necess√°rios

Configure no GitHub Actions:

```bash
GRAFANA_PASSWORD=<senha-forte>
SLACK_WEBHOOK_URL=<webhook-slack> # Opcional
WIF_PROVIDER=<workload-identity-provider>
WIF_SERVICE_ACCOUNT=<service-account-email>
GCP_PROJECT_ID=<project-id>
```

## üìà M√©tricas Coletadas

### Backend (Node.js + prom-client)

**M√©tricas HTTP:**
- `dx03_backend_http_request_duration_seconds` - Lat√™ncia das requisi√ß√µes
- `dx03_backend_http_requests_total` - Total de requisi√ß√µes por rota/m√©todo/status
- `dx03_backend_http_requests_in_progress` - Requisi√ß√µes simult√¢neas

**M√©tricas de Banco de Dados:**
- `dx03_backend_db_pool_connections` - Conex√µes do pool PostgreSQL
- `dx03_backend_db_query_duration_seconds` - Lat√™ncia das queries
- `dx03_backend_db_queries_total` - Total de queries por tipo/status

**M√©tricas do Node.js:**
- `dx03_backend_process_cpu_*` - Uso de CPU
- `dx03_backend_process_resident_memory_bytes` - Mem√≥ria utilizada
- `dx03_backend_nodejs_eventloop_lag_*` - Event loop lag
- `dx03_backend_nodejs_heap_*` - Heap do V8

### Kubernetes (Kube State Metrics)

- Pods: status, restarts, CPU, mem√≥ria
- Deployments: replicas desejadas vs dispon√≠veis
- Services: endpoints ativos
- PersistentVolumes: capacidade e uso

### Infraestrutura (Google Cloud Monitoring)

- CPU e mem√≥ria dos nodes
- Network I/O dos nodes
- Disk I/O e utiliza√ß√£o
- M√©tricas do Load Balancer

## üñ•Ô∏è Acesso aos Componentes

### Grafana

**Via Port-Forward (Recomendado):**
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3001:80
```

Acesse: http://localhost:3001

**Credenciais padr√£o:**
- Username: `admin`
- Password: Definida no secret `GRAFANA_PASSWORD`

**Reset de senha se necess√°rio:**
```bash
kubectl exec -n monitoring deployment/kube-prometheus-stack-grafana -- \
  grafana-cli admin reset-admin-password NovaSenha123
```

### Prometheus

**Via Port-Forward:**
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
```

Acesse: http://localhost:9090

**Endpoints √∫teis:**
- `/targets` - Status dos targets sendo monitorados
- `/graph` - Query interface
- `/alerts` - Alertas ativos

### Alertmanager

**Via Port-Forward:**
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093
```

Acesse: http://localhost:9093

## üìä Dashboards Dispon√≠veis

### 1. DX03 Application Dashboard

M√©tricas espec√≠ficas da aplica√ß√£o DX03:
- **Running Pods** - Total de pods em execu√ß√£o
- **CPU Usage by Pod** - Uso de CPU por pod
- **Memory Usage by Pod** - Uso de mem√≥ria por pod
- **Network Traffic** - Tr√°fego de rede (incoming/outgoing)
- **Pod Information** - Tabela com detalhes dos pods

### 2. GKE Nodes Dashboard (Cloud Monitoring)

M√©tricas dos nodes do GKE Autopilot:
- **Node CPU Utilization** - Utiliza√ß√£o de CPU dos nodes
- **Node Memory Utilization** - Utiliza√ß√£o de mem√≥ria dos nodes
- **CPU Usage by Node** - S√©rie temporal de CPU por node
- **Memory Allocatable** - Mem√≥ria dispon√≠vel por node
- **Network Traffic** - Tr√°fego enviado/recebido pelos nodes

### 3. Dashboards Built-in

O Grafana vem com v√°rios dashboards pr√©-configurados:
- Kubernetes Cluster Overview
- Prometheus Stats
- Node Exporter (desabilitado no Autopilot)

## üîî Alertas

### Configura√ß√£o do Slack

‚úÖ **Status:** CONFIGURADO E ATIVO

1. Crie um Incoming Webhook no Slack:
   - https://api.slack.com/messaging/webhooks
   - Escolha o canal (#alerts recomendado)
   - Copie a URL do webhook

2. Configure o secret no GitHub:
   ```bash
   gh secret set SLACK_WEBHOOK_URL --body "https://hooks.slack.com/services/..."
   ```

3. Execute o workflow de upgrade:
   ```bash
   gh workflow run deploy-observability.yml --field action=upgrade
   ```

**Configura√ß√£o Atual:**
- ‚úÖ Secret `SLACK_WEBHOOK_URL` configurado no GitHub
- ‚úÖ Alertmanager reconfigurado com webhook v√°lido
- ‚úÖ Canal: `#dx03-alerts`
- ‚úÖ Notifica√ß√µes ativas para alertas critical, warning e info

### Alertas Configurados

**Alertas Cr√≠ticos (repeat: 4h):**
- Pod crashlooping
- Deployment com replicas insuficientes
- Node com recursos cr√≠ticos
- Database connection failures

**Alertas Warning (repeat: 12h):**
- Alto uso de CPU/mem√≥ria (>80%)
- Lat√™ncia elevada (P95 > 500ms)
- Error rate acima do threshold (>5%)
- Pool de conex√µes DB pr√≥ximo do limite (>80%)

**Alertas Info (repeat: 24h):**
- Eventos de scaling
- Deployment updates
- Certificate renewal notices

## üîç Queries √öteis (PromQL)

### M√©tricas da Aplica√ß√£o

```promql
# Request rate (req/s)
rate(dx03_backend_http_requests_total[5m])

# Lat√™ncia P95
histogram_quantile(0.95, 
  rate(dx03_backend_http_request_duration_seconds_bucket[5m])
)

# Error rate (%)
sum(rate(dx03_backend_http_requests_total{status_code=~"5.."}[5m])) / 
sum(rate(dx03_backend_http_requests_total[5m])) * 100

# Conex√µes ativas do pool DB
dx03_backend_db_pool_connections{state="total"}
```

### M√©tricas do Kubernetes

```promql
# CPU usage por pod
rate(container_cpu_usage_seconds_total{namespace="dx03-dev"}[5m])

# Mem√≥ria usage por pod
container_memory_usage_bytes{namespace="dx03-dev"}

# Pods rodando
kube_pod_status_phase{namespace="dx03-dev", phase="Running"}

# Pod restarts
rate(kube_pod_container_status_restarts_total{namespace="dx03-dev"}[1h])
```

## üì¶ Armazenamento e Reten√ß√£o

### Prometheus
- **Reten√ß√£o:** 7 dias
- **Storage:** 10Gi PVC (standard-rwo)
- **Intervalo de scrape:** 30s

### Grafana
- **Persistence:** 5Gi PVC (standard-rwo)
- **Dashboards:** Persistidos no PVC
- **Datasources:** Definidos no values.yaml

### Alertmanager
- **Storage:** 2Gi PVC (standard-rwo)
- **Reten√ß√£o:** Baseada na resolu√ß√£o dos alertas

## üõ†Ô∏è Troubleshooting

### Grafana n√£o carrega dados

1. Verifique se o Prometheus est√° coletando:
   ```bash
   kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
   ```
   Acesse http://localhost:9090/targets

2. Teste query no Prometheus:
   ```
   up{job="dx03-backend"}
   ```

3. Verifique datasource no Grafana:
   - Configuration ‚Üí Data Sources ‚Üí Prometheus
   - Clique em "Test" para validar conex√£o

### M√©tricas do backend n√£o aparecem

1. Verifique se o pod est√° expondo `/metrics`:
   ```bash
   kubectl exec -n dx03-dev deployment/dx03-backend -- \
     wget -q -O- http://localhost:3000/metrics
   ```

2. Verifique o ServiceMonitor:
   ```bash
   kubectl get servicemonitor -n monitoring
   kubectl describe servicemonitor dx03-backend -n monitoring
   ```

3. Verifique se o label `release=kube-prometheus-stack` est√° presente:
   ```bash
   kubectl get servicemonitor dx03-backend -n monitoring -o yaml
   ```

### Alertas n√£o chegam no Slack

1. Verifique se o Alertmanager est√° rodando:
   ```bash
   kubectl get pods -n monitoring | grep alertmanager
   ```

2. Verifique a configura√ß√£o:
   ```bash
   kubectl get secret alertmanager-config -n monitoring -o yaml
   ```

3. Teste o webhook manualmente:
   ```bash
   curl -X POST <SLACK_WEBHOOK_URL> \
     -H 'Content-Type: application/json' \
     -d '{"text":"Test from Alertmanager"}'
   ```

### Pods em CrashLoopBackOff

**Grafana crashando:**
- Problema comum: Plugins incompat√≠veis
- Solu√ß√£o: Remover plugins Angular deprecados do values.yaml

**Prometheus OOMKilled:**
- Aumentar recursos no values.yaml
- Reduzir reten√ß√£o de dados
- Diminuir frequ√™ncia de scrape

### GKE Autopilot: Recursos Negados

Componentes desabilitados por serem incompat√≠veis com Autopilot:
- ‚ùå Node Exporter (usa hostPath)
- ‚ùå Monitoring de kube-system
- ‚úÖ Alternativa: Google Cloud Monitoring

## üí∞ Custos Estimados

**GKE Autopilot (us-central1):**
- Prometheus pod: ~$15-20/m√™s
- Grafana pod: ~$5-10/m√™s
- Alertmanager pod: ~$3-5/m√™s
- **Total estimado:** $23-35/m√™s

**Persistent Storage:**
- 17Gi total (10Gi + 5Gi + 2Gi): ~$3-5/m√™s

**Total geral:** ~$26-40/m√™s

## üìö Refer√™ncias

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)
- [GKE Autopilot Documentation](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview)
- [prom-client (Node.js)](https://github.com/siimon/prom-client)

## üîÑ Upgrade e Manuten√ß√£o

### Upgrade do Helm Chart

```bash
# Verificar vers√£o atual
helm list -n monitoring

# Atualizar repo
helm repo update

# Upgrade via workflow
gh workflow run deploy-observability.yml --field action=upgrade
```

### Backup de Dashboards

Os dashboards s√£o persistidos no PVC do Grafana. Para backup adicional:

```bash
# Export via API
curl -H "Authorization: Bearer <api-key>" \
  http://localhost:3001/api/dashboards/db/<dashboard-slug> > backup.json
```

### Limpeza

Para remover completamente:

```bash
# Via workflow (mant√©m PVCs por padr√£o)
gh workflow run deploy-observability.yml --field action=uninstall

# Manual (incluindo PVCs)
helm uninstall kube-prometheus-stack -n monitoring
kubectl delete pvc -n monitoring --all
kubectl delete namespace monitoring
```

---

## üîê Seguran√ßa e HTTPS

### HTTPS Redirect

‚úÖ **Status:** ATIVO (implementado em 31/12/2025)

**Configura√ß√£o:**
- Recurso: `FrontendConfig` (GKE-specific)
- Comportamento: Redireciona todo tr√°fego HTTP ‚Üí HTTPS (301 Moved Permanently)
- Certificado: Google-managed SSL certificate (v√°lido at√© 29/03/2026)
- Aplicado em: Load Balancer Ingress

**Arquivos:**
- [k8s/frontend-config.yaml](https://github.com/maringelix/dx03/blob/master/k8s/frontend-config.yaml) - FrontendConfig resource
- [k8s/ingress.yaml](https://github.com/maringelix/dx03/blob/master/k8s/ingress.yaml) - Ingress com annotation

**Teste:**
```bash
# HTTP deve retornar 301 redirect
curl -I http://dx03.ddns.net

# Deve retornar:
# HTTP/1.1 301 Moved Permanently
# Location: https://dx03.ddns.net/
```

**Implementa√ß√£o:**
```yaml
# FrontendConfig
apiVersion: networking.gke.io/v1beta1
kind: FrontendConfig
metadata:
  name: dx03-frontend-config
spec:
  redirectToHttps:
    enabled: true
    responseCodeName: "MOVED_PERMANENTLY_DEFAULT"

# Ingress annotation
metadata:
  annotations:
    networking.gke.io/v1beta1.FrontendConfig: "dx03-frontend-config"
```

---

## üìä Hist√≥rico de Implementa√ß√£o

### Session 31/12/2025 - HTTPS Redirect + Slack Alerts

**Implementa√ß√µes:**
1. ‚úÖ **HTTPS Redirect** via FrontendConfig
   - Criado recurso FrontendConfig
   - Atualizado Ingress com annotation
   - Workflow deploy atualizado
   - Commits: `f44fccc`, `2e43c1f`

2. ‚úÖ **Slack Alertmanager**
   - Diagnosticado: Secret `SLACK_WEBHOOK_URL` n√£o configurado
   - Configurado secret no GitHub
   - Re-deploy observability executado
   - Alertmanager com webhook ativo
   - Run ID: `20612370155` (success)

**Resultados:**
- Todo tr√°fego HTTP ‚Üí HTTPS (301)
- Alertas Prometheus ‚Üí Slack `#dx03-alerts`
- Zero downtime nas mudan√ßas

---

**√öltima atualiza√ß√£o:** 31 de Dezembro de 2025
